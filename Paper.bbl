% Generated by IEEEtran.bst, version: 1.12 (2007/01/11)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{NovelConnectionist}
A.~Graves, M.~Liwicki, S.~Fernandez, R.~Bertolami, H.~Bunke, and
  J.~Schmidhuber, ``A novel connectionist system for unconstrained handwriting
  recognition,'' \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, vol.~31, no.~5, pp. 855--868, 2009.

\bibitem{Kurzweilinterview}
``Kurzweil {AI} interview with {JÃ¼rgen Schmidhuber} on the eight competitions
  won by his {Deep Learning} team,'' \url{
  http://www.kurzweilai.net/how-bio-inspired-deep-learning-keeps-winning-competitions},
  2009--2012, accessed: 08-05-2015.

\bibitem{NIPS2008_3449}
A.~Graves and J.~Schmidhuber, ``Offline handwriting recognition with
  multidimensional recurrent neural networks,'' in \emph{Advances in Neural
  Information Processing Systems 21}, D.~Koller, D.~Schuurmans, Y.~Bengio, and
  L.~Bottou, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Curran Associates,
  Inc., 2009, pp. 545--552.

\bibitem{Rumelhart:ExploMicroStructure}
D.~E. Rumelhart, J.~L. McClelland, and C.~PDP Research~Group, Eds.,
  \emph{Parallel Distributed Processing: Explorations in the Microstructure of
  Cognition, Vol. 1: Foundations}.\hskip 1em plus 0.5em minus 0.4em\relax
  Cambridge, MA, USA: MIT Press, 1986.

\bibitem{website:dataeverywhere}
``Data, data everywhere,'' \url{http://www.economist.com/node/15557443}, 2010,
  accessed 08-05-2015.

\bibitem{Kussul_improvedmethod}
E.~Kussul and T.~Baidyk, ``Improved method of handwritten digit recognition,''
  \emph{Image and Vision Computing 22}, 2004.

\bibitem{SVMSpeedPatternReg}
C.~Cortes and V.~Vapnik, ``Support vector machines speed pattern recognition,''
  \emph{Machine Learning}, 1995.

\bibitem{usinganalytic}
J.~C. Platt, ``Using analytic qp and sparseness to speed training of support
  vector machines,'' in \emph{IN NEURAL INFORMATION PROCESSING SYSTEMS
  11}.\hskip 1em plus 0.5em minus 0.4em\relax MIT Press, 1999, pp. 557--563.

\bibitem{FastKNNClusterBasedTrees}
B.~Zhang and S.~N. Srihari, ``Fast k -nearest neighbor classification using
  cluster-based trees,'' \emph{IEEE Transactions on Pattern Analysis and
  Machine Intelligence}, 2004.

\bibitem{MNISTHandWritten}
Y.~LeCun, C.~Cortes, and C.~Burges, ``Mnist handwritten digit database,''
  \url{http://yann.lecun.com/exdb/mnist/}, accessed: 08-05-2015.

\bibitem{Lecun98gradient-basedlearning}
Y.~Lecun, L.~Bottou, Y.~Bengio, and P.~Haffner, ``Gradient-based learning
  applied to document recognition,'' in \emph{Proceedings of the IEEE}, 1998,
  pp. 2278--2324.

\bibitem{MultiColummn}
D.~Ciresan, U.~Meier, and J.~Schmidhuber, ``Multi-column deep neural networks
  for image classification,'' in \emph{Proceedings of the 2012 IEEE Conference
  on Computer Vision and Pattern Recognition (CVPR)}, ser. CVPR '12.\hskip 1em
  plus 0.5em minus 0.4em\relax Washington, DC, USA: IEEE Computer Society,
  2012, pp. 3642--3649.

\bibitem{IGotMoreData}
M.~Xiao-Li and X.~Xie, ``I got more data, my model is more refined, but my
  estimator is getting worse! am i just dumb?.'' 2014.

\bibitem{WhenItSeem}
H.~Chernoff, \emph{When it Seems Desirable to Ignore Data.}, 1982.

\bibitem{perceptron}
F.~Rosenblatt, ``The perceptron--a perceiving and recognizing automaton,''
  Cornell Aeronautical Laboratory, Tech. Rep. 85-460-1, 1957.

\bibitem{Aizenberg07multilayerfeedforward}
I.~Aizenberg and C.~Moraga, ``Multilayer feedforward neural network based on
  multi-valued neurons (mlmvn) and a backpropagation learning algorithm,''
  \emph{SOFT COMPUTING}, vol.~11, no.~2, p. 2007, 2007.

\bibitem{CybenkoApproxSidmoid}
G.~Cybenko, ``{Approximation by superpositions of a sigmoidal function},''
  \emph{Mathematics of Control, Signals, and Systems (MCSS)}, vol.~2, no.~4,
  pp. 303--314, Dec. 1989.

\bibitem{FundermentalANN}
M.~H. Hassoun, \emph{Fundamentals of artificial neural networks}.\hskip 1em
  plus 0.5em minus 0.4em\relax MIT Press, 1995.

\bibitem{ZhangANNForecast}
G.~Zhang, B.~{Eddy Patuwo}, and H.~Y.~Micheal, ``Forecasting with artificial
  neural networks:: The state of the art,'' \emph{International Journal of
  Forecasting}, vol.~14, no.~1, pp. 35--62, Mar. 1998.

\bibitem{pittman2008handwriting}
J.~A. Pittman and M.~Manu, ``Handwriting recognition using neural networks,''
  Sep 23 2008.

\bibitem{buscema1998back}
M.~Buscema, ``Back propagation neural networks,'' \emph{Substance use \&
  misuse}, vol.~33, no.~2, pp. 233--270, 1998.

\bibitem{Leonard1990337ImproveBPNN}
J.~Leonard and M.~Kramer", ``Improvement of the backpropagation algorithm for
  training neural networks,'' \emph{Computers I\& Chemical Engineering},
  vol.~14, no.~3, pp. 337 -- 341, 1990.

\bibitem{Magoulas:1999:ImproveConvergenceBP}
G.~D. Magoulas, M.~N. Vrahatis, and G.~S. Androulakis, ``Improving the
  convergence of the backpropagation algorithm using learning rate adaptation
  methods,'' \emph{Neural Comput.}, vol.~11, no.~7, pp. 1769--1796, Oct. 1999.

\bibitem{Riedmiller93adirect}
M.~Riedmiller and H.~Braun, ``A direct adaptive method for faster
  backpropagation learning: The rprop algorithm,'' in \emph{IEEE INTERNATIONAL
  CONFERENCE ON NEURAL NETWORKS}, 1993, pp. 586--591.

\bibitem{dennis1977quasi}
J.~E. Dennis, Jr and J.~J. Mor{\'e}, ``Quasi-newton methods, motivation and
  theory,'' \emph{SIAM review}, vol.~19, no.~1, pp. 46--89, 1977.

\bibitem{grippo1997globally}
L.~Grippo and S.~Lucidi, ``A globally convergent version of the polak-ribiere
  conjugate gradient method,'' \emph{Mathematical Programming}, vol.~78, no.~3,
  pp. 375--391, 1997.

\bibitem{wei2000new}
Z.~Wei, L.~Qi, and S.~Ito, ``New step-size rules for optimization problems,''
  \emph{Department of Mathematics and Information Science, Guangxi University,
  Nanning, Guangxi, People's Republic of China}, 2000.

\bibitem{Geman:1992:NNBBiasDilemma}
S.~Geman, E.~Bienenstock, and R.~Doursat, ``Neural networks and the
  bias/variance dilemma,'' \emph{Neural Comput.}, vol.~4, no.~1, pp. 1--58,
  Jan. 1992.

\bibitem{BiasVarianceDecomposition}
``Bias variance decomposition.'' in \emph{Encyclopedia of Machine Learning},
  C.~Sammut and G.~I. Webb, Eds.\hskip 1em plus 0.5em minus 0.4em\relax
  Springer, 2010, pp. 100--101.

\bibitem{Meng94}
X.~L. Meng, ``{Multiple-imputation inferences with uncongenial sources of
  input},'' \emph{Statistical Science}, vol.~9, pp. 538--558, 1994.

\bibitem{Hornik_Universal_appro}
K.~Hornik, M.~Stinchcombe, and H.~White, ``Universal approximation of an
  unknown mapping and its derivatives using multilayer feedforward networks,''
  \emph{Neural Netw.}, vol.~3, no.~5, pp. 551--560, Oct. 1990.

\bibitem{Polak1969}
E.~Polak and R.~G, ``Note sur la convergence de {methodes} de directions
  {conjuguees},'' \emph{ESAIM: Mathematical Modelling and Numerical Analysis -
  Modelisation Mathematique et Analyse Numerique}, vol.~3, no.~R1, pp. 35--43,
  1969.

\end{thebibliography}
